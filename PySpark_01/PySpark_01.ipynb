{"cells":[{"cell_type":"code","source":["Create Spark Session \nRead DataSets From csv\nVisualize DataFrame\nSelect Column From DataFrame\nAdding Columns In DataFrame\nDrop Columns\nAdding Rows In DataFrame\nRename Columns \nCheck DataTypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9f68e4ab-c087-40cb-a901-f02101e36e11","inputWidgets":{},"title":"Topics"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c4524133-c489-4aa4-91d2-b47512282aa9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PySpark applications start with initializing SparkSession which is the entry point of PySpark\nfrom pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"91b3ff30-2a27-41cf-9567-74f319b2e835","inputWidgets":{},"title":"Create Spark Session "}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark=SparkSession.builder.appName('DataFrame').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"72187928-18f0-4d44-8017-dd3fee7e59c5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7a8cb935-e5dd-45fd-8787-6c211f19fcd5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1195182798048843#setting/sparkui/0505-042506-y1w9yrsj/driver-2017854775710208258\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"/?o=1195182798048843#setting/sparkui/0505-042506-y1w9yrsj/driver-2017854775710208258\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[8]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Databricks Shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]}}],"execution_count":0},{"cell_type":"code","source":["df_pyspark=spark.read.csv('/FileStore/tables/test1.csv')\n# Notice how output says c0(column0) is string,etc (does'nt mention proper name of column)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"952e8638-062a-43ec-a8af-0416118932f5","inputWidgets":{},"title":"Read DataSets"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_pyspark=spark.read.option('header','true').csv('/FileStore/tables/test1.csv')\n# Column Headers Are Visible Now"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"451da2e6-4fff-477e-9ac1-d05aafdb68f4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(df_pyspark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8b31895d-be9f-435e-a4a4-0593ec7c98ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[15]: pyspark.sql.dataframe.DataFrame"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"703ce80c-dc79-4f56-a1fd-0da8b539c400","inputWidgets":{},"title":"Visualize DataFrame"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark.head(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2e3ed411-f0d1-45fc-b366-54da48addba1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[29]: [Row(Name='Krish', age=31, Experience=10, Salary=30000),\n Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000)]"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark.printSchema()\n# Notice age, salary and experience Column Are String,Instead Of Int Which Was Provided as Int In .csv File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6542676e-a2e8-4292-a06b-706b47cdf608","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- Experience: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# InferSchema = True Solves Above Problem\ndf_pyspark=spark.read.option('header','true').csv('/FileStore/tables/test1.csv',inferSchema=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d7a5ea62-388e-41bb-b29e-33745d512de2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_pyspark.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5d86e59e-cedd-4618-8868-29178b59d144","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark.select(['Name']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"03bbd996-ca77-4a35-800f-36644c429f75","inputWidgets":{},"title":"Select Column From DataFrame"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+\n|     Name|\n+---------+\n|    Krish|\n|Sudhanshu|\n|    Sunny|\n|     Paul|\n|   Harsha|\n|  Shubham|\n+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark.select(['Name','Experience']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"980bcfbf-2e24-41d2-b214-27e8516fd45a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------+\n|     Name|Experience|\n+---------+----------+\n|    Krish|        10|\n|Sudhanshu|         8|\n|    Sunny|         4|\n|     Paul|         3|\n|   Harsha|         1|\n|  Shubham|         2|\n+---------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Adding a Column Which Tells Age Of Every Person After 5 years\ndf_pyspark = df_pyspark.withColumn('Age After 5 Years',df_pyspark['age']+5)\n\n#  PySpark lit() function is used to add a constant value to a DataFrame column\nfrom pyspark.sql.functions import lit\ndf_pyspark = df_pyspark.withColumn(\"Country\", lit(\"INDIA\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"efac625d-76dd-48e4-915a-9f7479410bbb","inputWidgets":{},"title":"Adding Columns In DataFrame"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_pyspark.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e87f023b-7156-47fc-b666-e3922ecc6dea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---+----------+------+-----------------+-------+\n|     Name|age|Experience|Salary|Age After 5 Years|Country|\n+---------+---+----------+------+-----------------+-------+\n|    Krish| 31|        10| 30000|               36|  INDIA|\n|Sudhanshu| 30|         8| 25000|               35|  INDIA|\n|    Sunny| 29|         4| 20000|               34|  INDIA|\n|     Paul| 24|         3| 20000|               29|  INDIA|\n|   Harsha| 21|         1| 15000|               26|  INDIA|\n|  Shubham| 23|         2| 18000|               28|  INDIA|\n+---------+---+----------+------+-----------------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark=df_pyspark.drop('Age After 5 Years')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ea5b64b3-1c6a-4797-85d0-f9ebb745cd82","inputWidgets":{},"title":"Drop Columns"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_pyspark.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c9b0a435-8c2a-446c-8e1c-7a8c472620da","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---+----------+------+-------+\n|     Name|age|Experience|Salary|Country|\n+---------+---+----------+------+-------+\n|    Krish| 31|        10| 30000|  INDIA|\n|Sudhanshu| 30|         8| 25000|  INDIA|\n|    Sunny| 29|         4| 20000|  INDIA|\n|     Paul| 24|         3| 20000|  INDIA|\n|   Harsha| 21|         1| 15000|  INDIA|\n|  Shubham| 23|         2| 18000|  INDIA|\n+---------+---+----------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["columns = ['Name', 'age', 'Experience', 'Salary','Country']\nnewRow = spark.createDataFrame([('Pragya','21','2','400000','INDIA')], columns )\nappended = df_pyspark.union(newRow)\nappended.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d701beb2-aa87-4b85-8fec-5d45e8547ded","inputWidgets":{},"title":"Adding Rows Into DataFrame"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---+----------+------+-------+\n|     Name|age|Experience|Salary|Country|\n+---------+---+----------+------+-------+\n|    Krish| 31|        10| 30000|  INDIA|\n|Sudhanshu| 30|         8| 25000|  INDIA|\n|    Sunny| 29|         4| 20000|  INDIA|\n|     Paul| 24|         3| 20000|  INDIA|\n|   Harsha| 21|         1| 15000|  INDIA|\n|  Shubham| 23|         2| 18000|  INDIA|\n|   Pragya| 21|         2|400000|  INDIA|\n+---------+---+----------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["adf_pyspark.withColumnRenamed('Name','FirstName').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e79feaad-ad08-4cb5-973b-8e28b20f06d1","inputWidgets":{},"title":"Rename Columns "}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---+----------+------+-------+\n|FirstName|age|Experience|Salary|Country|\n+---------+---+----------+------+-------+\n|    Krish| 31|        10| 30000|  INDIA|\n|Sudhanshu| 30|         8| 25000|  INDIA|\n|    Sunny| 29|         4| 20000|  INDIA|\n|     Paul| 24|         3| 20000|  INDIA|\n|   Harsha| 21|         1| 15000|  INDIA|\n|  Shubham| 23|         2| 18000|  INDIA|\n+---------+---+----------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pyspark.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"de3fda7f-347b-4317-bfb4-7604d59de2eb","inputWidgets":{},"title":"Check DataTypes"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[40]: [('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark_01","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3078958713759748}},"nbformat":4,"nbformat_minor":0}
